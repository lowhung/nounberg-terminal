# Nounberg Terminal

A realâ€‘time Nouns DAO auction tracker that **indexes** onâ€‘chain events, **enriches** them with offâ€‘chain context,
produces concise humanâ€‘readable headlines, and serves both a paginated REST feed and a live WebSocket streamâ€”all
bootstrapped by a single `make start`.

---

## ğŸ—ï¸â€¯Architecture Overview

*Diagram shows event flow and external integrations; each service is detailed below.*

### Services

| Package                            | TechÂ Stack                   | Responsibility                                                                                                                                                                                                                                      | Notable Features                                                                                                                                                                                           |
|------------------------------------|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Indexer** (`packages/indexer`)   | Ponder (TypeScript)          | Streams `AuctionCreated`, `AuctionBid`, `AuctionSettled` from mainâ€‘net and writes a **base event** row keyed by the onâ€‘chain `event.id`.                                                                                                            | â€¢ Reâ€‘org rollback via Ponder hooksâ€¢ Builtâ€‘in GraphQL explorer atÂ `:42069`â€¢ `ONÂ CONFLICT` safeguard in case of duplicate or rolledâ€‘back logsâ€¢ Enqueues enrichment jobs after insert                         |
| **Queue** (`packages/queue`)       | BullMQÂ +Â Redis               | Receives enrichment jobs; exposes Prometheus metrics & `/health`.                                                                                                                                                                                   | â€¢ Lockâ€‘manager sets prevent duplicate ENS / price cache fetchesâ€¢ Completed jobs are **immediately removed** to keep Redis lean during spikes (configurable retention window if forensic traces are needed) |
| **Workers** (`packages/queue`)     | BullMQ worker pool (NodeÂ TS) | Fetches jobs, enriches with USD price, ENS, thumbnail, and headline, then **GETs + UPDATEs** the event row (no upsert). Inserts are left solely to the Indexer so Ponder can manage rollbacks; if the row isnâ€™t present yet the job simply retries. | â€¢ Redis cache (48â€¯h ENS, 1â€¯h price) behind lockâ€‘manager keys so only one worker populates a missâ€¢ Alchemy historical price APIâ€¢ Thumbnails via `https://noun.pics/{nounId}`                                |
| **API** (`packages/api`)           | Hono.js                      | Cursorâ€‘based REST `/api/events` and WebSocket `/ws`; relays Postgres `LISTEN/NOTIFY`.                                                                                                                                                               | â€¢ Single service handles HTTP + WS for PoC; for prod a dedicated realtime gateway is preferredâ€¢ **No OpenAPI docs**â€”endpoint schema described in README                                                    |
| **Frontend** (`packages/frontend`) | ReactÂ +Â TailwindCSS          | Lets users: (1) watch the live stream, (2) **scroll forward** through history via cursor pagination, (3) filter by eventâ€¯type or nounâ€¯ID.                                                                                                           | â€“                                                                                                                                                                                                          |
| **Datastores**                     | PostgreSQL / Redis           | Truth store and cache/queue backend.                                                                                                                                                                                                                | Triggers emit `NOTIFY auction_events` used by API.                                                                                                                                                         |

---

## ğŸ“‚â€¯Data Model

All events reside in a single **`auction_events`** table keyed by the \*\*onâ€‘chain \*\***`event_id`** (transaction
hash + log index, exposed by Ponder as `event.id`). Columns:

| column            | type        | purpose                          |
|-------------------|-------------|----------------------------------|
| `event_id`        | textÂ PK     | deterministic unique id from log |
| `event_type`      | enum        | `created`, `bid`, `settled`      |
| `block_timestamp` | timestamptz | used for cursor pagination       |
| `headline`        | text        | human string                     |
| â€¦                 | â€¦           | other enrichment fields          |

**Why denormalise?**

* The feed is consumed chronologically; explicit columns (many nullable) avoid costly joins or unions.
* Cursor pagination using `block_timestampÂ DESC,Â event_idÂ DESC` remains stable even while new rows stream in.
* Updates are simple `UPDATE â€¦ WHERE event_id = ?`â€”no race with inserts.

If analytical or relational queries grow, a normalised shadow schema or materialised view can be built without touching
the hot path.

**Indexes in use**(defined via Ponder):

* `block_timestamp` (singleâ€‘column) â€” drives cursor pagination.
* `type`, `nounId` (singleâ€‘column) â€” quick filters in the REST endpoint.
* Compound indexes: `(type, block_timestamp)`, `(nounId, block_timestamp)`, `(type, nounId, block_timestamp)` â€” cover
  the common â€œshow bids for noun #721â€ and similar queries.

Full schema lives in `packages/indexer/ponder/schema.ts`; trimmed here for readability.

---

## ğŸ—„ï¸â€¯Caching Strategy & Lock Management

| Item          | TTL                                                                                   | Lock Key Example        |
|---------------|---------------------------------------------------------------------------------------|-------------------------|
| ENS name      | 48â€¯h                                                                                  | `lock:ens:0xd8dA6Bâ€¦`    |
| ETH/USD price | â‰¤â€¯1â€¯h old â†’ **24â€¯h TTL**<br>1â€¯hâ€¯â€“â€¯24â€¯h old â†’ **7â€¯d TTL**<br>ï¼â€¯24â€¯h old â†’ **30â€¯d TTL** | `lock:price:<hour-iso>` |

Workers acquire a **Redis `SETNX` lock** before external lookâ€‘ups; the first worker populates the cache, others read the
cached valueâ€”eliminating duplicate calls without heavy coordination.

> **Future optimisation:** `SETNX` is simple but singleâ€‘instance. For multiâ€‘node resilience we could switch to
> a [Redlock](https://redis.io/docs/interact/locks/)â€‘style algorithm or use a small Lua script that performs â€œget or fetch
> then setâ€ atomically.

---

## ğŸ”„â€¯Endâ€‘toâ€‘EndÂ Flow

1. **Detect**â€” Indexer writes base row keyed by `event_id`.
2. **Enqueue**â€” Job submitted to BullMQ.
3. **Lock & Fetch**â€” Worker locks ENS/price keys and fetches as needed.
4. **Update**â€” Worker `UPDATE`s row with enrichment (no UPSERT needed because row already exists).
5. **Notify**â€” `NOTIFY auction_events` with `event_id`.
6. **Broadcast**â€” API pushes JSON to WebSocket clients; REST reflects immediately.

---

## ğŸš€â€¯QuickÂ Start

```bash
#Â Prepare env
cp .env.example .env
# add PONDER_RPC_URL_1 and ALCHEMY_API_KEY

make start                 # build & launch full stack
```

| URL                      | Purpose                        |
|--------------------------|--------------------------------|
| `http://localhost:8080`  | Frontend demo (live & history) |
| `http://localhost:3000`  | REST API                       |
| `ws://localhost:3000/ws` | WebSocket feed                 |
| `http://localhost:42069` | Ponder GraphQL explorer        |

---

## ğŸ“¡â€¯APIÂ Reference

### GETÂ `/api/events`

```http
GET /api/events?cursor=2025â€‘05â€‘29T16:0xabcâ€¦&limit=20&type=bid&nounId=721
```

*Query params*

* `cursor` â€” opaque string `block_timestamp:event_id`
* `limit`â€” up to 100
* `type`â€” optional `created|bid|settled`
* `nounId`â€” optional filter

Returns newestâ€‘first events; exact JSON schema is documented inline in code.

### WebSocketÂ `/ws`

```jsonc
// Client â†’ Server
{ "type": "subscribe", "filters": { "nounId": 721 } }

// Server â†’ Client
{ "type": "event", "data": { /* event object */ } }
```

### Health Endpoints

```
GET /api/health        # API liveness
GET /queue/health      # Queue & worker pool
```

---

## ğŸ§ªâ€¯Testing with Foundry

Topâ€‘level `make test` orchestrates:

1. **startâ€‘anvil** â€” mainnet fork.
2. **deploy** â€” deploys test contracts + simulates auction.
3. **testâ€‘ponder** â€” asserts indexer/worker output.

Makefile snippets are included below for reference.

---

## ğŸ§ â€¯Architectural Notes & Takeâ€‘aways

### Blockchain Tooling

* **Foundry (Forge + Anvil)** delivers quick, repeatable mainnetâ€‘fork testsâ€”something absent in Cardanoâ€™s toolâ€‘chain and
  a huge velocity boost when validating complex auction flows.
* **Open ABIs (Etherscan, Sourcify)** mean event signatures and calldata layouts are one search away, so logâ€‘decoding
  and debugging are markedly faster than in ecosystems where contract interfaces are opaque.

### Service Boundaries

* **Dedicated API vs Indexer**â€” REST/WebSocket traffic runs in its own process while chain ingestion remains isolated;
  this separation simplifies scaling decisions (CPU for workers, I/O for API) and keeps failure domains narrow.
* **Worker isolation**â€” Enrichment is CPUâ€‘bound and parallelâ€‘friendly; running it in a discrete container group lets
  Kubernetes HPA scale based on queue depth without impacting request latency on the API.

### Internal Communication

* **Why gRPC (future)**â€” Stronglyâ€‘typed protobuf contracts and efficient streaming are ideal once service count grows.
  Drawbacks: schema evolution rules (appendâ€‘only fields) and CI plumbing to regenerate types. REST is fine for the
  current twoâ€‘hop topology, but a migration path is planned.

### Realâ€‘Time Delivery

* **PostgreSQL LISTEN/NOTIFY** ensures notifications fire only after the row is committedâ€”no dualâ€‘write race that
  sometimes occurs with Redis Pub/Sub.
* **WebSocket in Hono vs dedicated gateway** â€” Honoâ€™s WS support works but needs explicit upgrade handling. For
  production, a separate realtime service (Fastify, native `ws`, uWebSockets.js) would simplify longâ€‘lived connection
  management and keep the REST layer stateless.
* **Stateful connections** â€” The API process currently holds all active WebSocket clients in memory. That works for a
  singleâ€‘instance PoC, but horizontal scaling would require either sticky sessions **or** an external broker (e.g.,
  Redis Pub/Sub, NATS) so any replica can broadcast to all clients.
* WebSocket chosen over SSE for bidirectional features (future auth/filter commands). Rateâ€‘limiting and perâ€‘user filters
  are on the roadmap.

### Upsert vs Update

* **Current flow:** Indexer performs the insert via Ponderâ€™s Store API, which buffers writes inâ€‘memory during historical
  sync and flushes them to PostgreSQL with COPYâ€”fast and reâ€‘orgâ€‘safe. Workers then issue parameterised UPDATEs to append
  enrichment. This keeps insert logic in one placeâ€”the component that understands rollbacksâ€”and avoids crossâ€‘service
  contention.

* **Scaling:** In production, more worker replicas (Docker Compose â–¶ --scale workers=N, ECS service, etc.) consume the
  same queue. Because every job ultimately resolves to a single UPDATE, contention is minimal and database locks remain
  shortâ€‘lived. BullMQâ€™s batch jobs / pipelines can further boost throughput by letting each worker acknowledge multiple
  completed jobs in one roundâ€‘trip.

* **Why not use Store API in workers?**  Store API is part of Ponderâ€™s runtime and only available inside its hooks; the drizzle it exposes is read-only. The
  workers run as a standalone Node service with their own connection pool. They therefore use parameterised raw SQL,
  which also keeps transaction boundaries explicit and straightforward.

* **Future improvement:** Introduce a small typeâ€‘safe query helper (e.g., wrapping pg with Zod schemas) or migrate to an
  ORM such as Drizzle once it supports LISTEN/NOTIFY. This would align typing across services without pulling in the
  Ponderâ€‘specific Store API, which as mentioned above is unavailable outside the Indexer context.

### Observability

* **Prometheus metrics** already exposed by Indexer and Queue/Worker; dashboards live under `monitoring/`. API metrics
  will be added after the WebSocket refactor.
* **Structured logging** â€” Plan to extract a shared `@nounberg/logger` (pino wrapper, similar to Ponderâ€™s internal
  logger) so every service outputs JSON logs with trace/context fields, making aggregation in Loki or Elasticsearch
  straightforward.

## ğŸ”®â€¯Future Work

* **EIPâ€‘4361 (Signâ€‘In with Ethereum)** for authenticated WebSocket streams.
* Migrate internal REST to **gRPC**.
* Implement requestâ€‘level **rateâ€‘limiting** once auth is in place.
* Extract shared libs (`@nounberg/logger`, types, SQL) into workspace package.
* Seed full historical ETH price table or run internal oracle.

---

## ğŸ› â€¯Makefile Commands

Below are the **userâ€‘facing commands** youâ€™ll run most often; full recipes are in each Makefile.

| Command             | Mode     | What it does                                                                               |
|---------------------|----------|--------------------------------------------------------------------------------------------|
| `make start`        | **Prod** | Build and launch the complete stack (all services + infra)                                 |
| `make dev`          | **Dev**  | Spin up Postgres & Redis only; you then run one or more `make devâ€‘<service>` targets below |
| `make dev-indexer`  | **Dev**  | Start the indexer with liveâ€‘reload, relying on the shared infra from `make dev`            |
| `make dev-api`      | **Dev**  | Start the RESTÂ +Â WebSocket API                                                             |
| `make dev-workers`  | **Dev**  | Launch the worker pool & queue API                                                         |
| `make dev-frontend` | **Dev**  | Start the React frontâ€‘end (Vite dev server)                                                |
| `make test`         | **Test** | Endâ€‘toâ€‘end Foundry run (fork mainnet, deploy contracts, assert indexing)                   |
| `make stop`         | â€”        | Bring down all running containers                                                          |
| `make clean`        | â€”        | Stop containers and prune Docker artifacts                                                 |

For contractâ€‘level tests the **Foundry Makefile** offers:

| Command            | Purpose                                                            |
|--------------------|--------------------------------------------------------------------|
| `make start-anvil` | Spin up an Anvil mainnet fork (side stack)                         |
| `make deploy`      | Build a small deployer image and simulate a complete auction cycle |
| `make test-ponder` | Run Ponder against that fork and verify events                     |
| `make test`        | Shortcut: `start-anvil âœ deploy âœ test-ponder`                     |


# Nounberg Terminal

A realâ€‘time **Nouns DAO** auction tracker that **indexes** onâ€‘chain events, **enriches** them with offâ€‘chain context, produces concise humanâ€‘readable headlines, and serves both a paginated REST feed **and** a live WebSocket stream.

---

## ğŸ—ï¸Â Architecture Overview

![Nounberg Terminal Architecture](./nounberg-excalidraw.png)

*Diagram shows event flow and external integrations; each service is detailed below.*

### Services

| Package                             | TechÂ Stack                   | Responsibility                                                                                                                                                                                                                                      | NotableÂ Features                                                                                                                                                                                               |
| ----------------------------------- | ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Indexer**<br>`packages/indexer`   | Ponder (TypeScript)          | Streams `AuctionCreated`, `AuctionBid`, `AuctionSettled` from mainâ€‘net and writes a **base event** row keyed by the onâ€‘chain `event.id`.                                                                                                            | â€¢Â Reâ€‘org rollback via Ponder hooks<br>â€¢Â Builtâ€‘in GraphQL explorer at `:42069`<br>â€¢Â `ONÂ CONFLICT` safeguard in case of duplicate or rolledâ€‘back logs<br>â€¢Â Enqueues enrichment jobs after insert                 |
| **Queue**<br>`packages/queue`       | BullMQÂ +Â Redis               | Receives enrichment jobs; exposes Prometheus metrics & `/health`.                                                                                                                                                                                   | â€¢Â Lockâ€‘manager sets prevent duplicate ENS / price cache fetches (called "thundering herd")<br>â€¢Â Completed jobs are **immediately removed** to keep Redis lean during spikes (configurable retention window if forensic traces are needed) |
| **Workers**<br>`packages/queue`     | BullMQ workerâ€‘pool (NodeÂ TS) | Fetches jobs, enriches with USD price, ENS, thumbnail, and headline, then **GETsÂ +Â UPDATEs** the event row (no upsert). Inserts are left solely to the Indexer so Ponder can manage rollbacks; if the row isnâ€™t present yet the job simply retries with exponential backoff. | â€¢Â Redis cache (48â€¯h ENS,Â 1â€¯h price) behind lockâ€‘manager keys so only one worker populates a miss<br>â€¢Â Alchemy historical price API<br>â€¢Â Thumbnails via `https://noun.pics/{nounId}`                            |
| **API**<br>`packages/api`           | Hono.js                      | Cursorâ€‘based REST `/api/events` and WebSocket `/ws`; relays Postgres `LISTEN/NOTIFY`.                                                                                                                                                               | â€¢Â Single service handles HTTP **and** WS for PoC; for prod a dedicated realtime gateway is preferred<br>â€¢Â **No OpenAPI docs**Â â€” endpoint schema described in README                                            |
| **Frontend**<br>`packages/frontend` | ReactÂ +Â TailwindCSS          | Lets users (1) watch the live stream, (2) **scroll forward** through history via cursor pagination, (3) filter by eventÂ type or nounÂ ID.                                                                                                            | â€“                                                                                                                                                                                                              |
| **Datastores**                      | PostgreSQL / Redis           | Truth store and cache/queue backend.                                                                                                                                                                                                                | â€¢Â Triggers emit `NOTIFY auction_updated` consumed by the API                                                                                                                                                    |

---

## ğŸ“‚Â Data Model

All events reside in a single **`auction_events`** table keyed by the **onâ€‘chain** `event_id` (transaction hashÂ +Â log index, exposed by Ponder as `event.id`).

| column            | type               | purpose                          |
| ----------------- | ------------------ | -------------------------------- |
| `event_id`        | `textÂ PRIMARYÂ KEY` | unique id frome event            |
| `event_type`      | `text`             | `created`, `bid`, `settled`      |
| `block_timestamp` | `numeric(78)`      | used for cursor pagination       |
| `headline`        | `text`             | humanâ€‘readable summary           |
| â€¦                 | â€¦                  | other enrichment fields          |


### WhyÂ denormalise?

* The feed is consumed chronologically; explicit columns (many nullable) avoid costly joins or unions.
* Cursor pagination using **`block_timestampÂ DESC,Â event_idÂ DESC`** is fairly stable for this singleâ€‘collection feed. To be honest, this works well only because it's a single contract table, but if you had to support additional contracts, timestamps from parallel streams will interleave. You'd then either:
â€¯Â Â â€¢ keep perâ€‘contract tables and paginate within each, or
â€¯Â Â â€¢ introduce a composite key (collection_id, block_timestamp, event_id) and paginate per collection_id. This avoids â€œhot shardâ€ skew, and keeps cursor order deterministic.
* Updates are simple `UPDATE â€¦ WHERE event_id = ?`Â â€” no race with inserts.

Lots of avenues to discuss with denormalizing the data if more analytical or relational queries are required -- you can build out a normalised shadow schema, or materialised view without touching the hot path.

**Indexes in use** (defined via Ponder):

* `block_timestamp` (singleâ€‘column)Â â€” drives cursor pagination.
* `type`,Â `nounId` (singleâ€‘column)Â â€” quick filters in the REST endpoint.
* Compound: `(type, block_timestamp)`, `(nounId, block_timestamp)`, `(type, nounId, block_timestamp)`Â â€” cover common â€œshow bids for nounÂ #721â€â€‘style queries.

Full schema lives in `packages/indexer/ponder/schema.ts`; trimmed here for readability.

---

## ğŸ—„ï¸Â CachingÂ StrategyÂ &Â LockÂ Management

| Item          | TTL                                                                                   | LockÂ KeyÂ Example        |
| ------------- | ------------------------------------------------------------------------------------- | ----------------------- |
| ENS name      | 48â€¯h                                                                                  | `lock:ens:0xd8dA6Bâ€¦`    |
| ETH/USD price | â‰¤â€¯1â€¯h old â†’ **24â€¯h TTL**<br>1â€¯hÂ â€“Â 24â€¯h old â†’ **7â€¯d TTL**<br>ï¼â€¯24â€¯h old â†’ **30â€¯d TTL** | `lock:price:<hourâ€‘iso>` |

Workers acquire a **Redis `SETNX` lock** before external lookâ€‘ups; the first worker populates the cache, others read the cached valueÂ â€” eliminating duplicate calls ("thundering herd".

> **Future optimisation:** `SETNX` is simple but singleâ€‘instance. For multiâ€‘node resilience we could switch to a [Redlock](https://redis.io/docs/latest/develop/use/patterns/distributed-locks/)â€‘style algorithm or use a small Lua script that performs â€œget or fetch then setâ€ atomically.

---

## ğŸ”„Â Endâ€‘toâ€‘EndÂ Flow

1. **Detect**Â â€” Indexer writes base row keyed by `event_id`.
2. **Enqueue**Â â€” Job submitted to Queue API.
3. **Lock / SetÂ &Â Fetch**Â â€” Workers use a single instance lock manager where only one worker can set the price for a rounded timestamp (round to nearest hour for demo) at a time. Workers that can't acquire the lock poll the cache key directly (rather than waiting for the lock to release), ensuring they get the data when lock-holding worker populates it.
4. **Update**Â â€” Worker `UPDATE`s row with enrichment (no UPSERT needed because row already exists).
5. **Notify**Â â€” `NOTIFY auction_updated` with `event_id`.
6. **Broadcast**Â â€” API pushes JSON to WebSocket clients; REST reflects immediately.

---

## ğŸš€Â QuickÂ Start

```bash
#Â Prepare env
cp .env.example .env       # add PONDER_RPC_URL_1 and ALCHEMY_API_KEY

make start                 # buildÂ &Â launch full stack
```

| URL                      | Purpose                        |
| ------------------------ | ------------------------------ |
| `http://localhost:8080`  | Frontend demo (liveÂ &Â history) |
| `http://localhost:3000`  | REST API                       |
| `ws://localhost:3000/ws` | WebSocket feed                 |
| `ws://localhost:3001`    | Queue API                      |
| `http://localhost:42069` | Ponder GraphQL explorer        |

---

## ğŸ“¡Â APIÂ Reference

### GETÂ `/api/events`

```http
GET /api/events?cursor=1708356695&limit=20&type=bid&nounId=721
```

**QueryÂ params**

| param    | description                              |
| -------- | ---------------------------------------- |
| `cursor` | opaque string `block_timestamp` |
| `limit`  | up to 100                                |
| `type`   | optional `created\|bid\|settled`         |
| `nounId` | optional filter                          |

Returns newestâ€‘first events; exact JSON schema is documented inline in code.

### WebSocketÂ `/ws`

```jsonc
// ClientÂ â†’Â Server
{ "type": "subscribe" }

// ServerÂ â†’Â Client
{ "type": "event", "data": { /* event object */ } }
```

### HealthÂ Endpoints

```text
GET localhost:3000/api/health        # API liveness
GET localhost:3001/health            # QueueÂ &Â worker pool
```

---

## ğŸ§ªÂ TestingÂ withÂ Foundry

Topâ€‘level **`make test`** orchestrates:

1. **startâ€‘anvil**Â â€” mainnet fork.
2. **deploy**Â â€” deploys test contractsÂ + simulates auction.
3. **testâ€‘ponder**Â â€” asserts indexer/worker output.

https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExOTI2cWZkeHp3NnBvZnY5aGswYjhjeWhrZmplMWN2MXowM24xbjI3YyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/nlHN6K6xkmFmaUsmyE/giphy.gif

Makefile snippets are included below for reference.

---

## ğŸ§ Â ArchitecturalÂ NotesÂ &Â Takeâ€‘aways

### BlockchainÂ Tooling

* **Foundry (ForgeÂ +Â Anvil)** stack was really easy to set up via Docker, and provides repeatable mainnetâ€‘fork tests for deploying the mock contract, and broadcasting events using Forge. Cardano does have a testnet with a faucet, but I'm not aware of a tool like Foundry + Anvil + Forge for Cardano. The documentation is clear and I'm looking forward to using it more.
* **OpenÂ ABIs (Etherscan):** one thing I really appreciate about Etherscan is that the underly contracts code is shared in the scanner. Having a central place to observe the contracts, receipt logs, event signatures and calldata layouts etc for Noun House was super easy. Cardano doesn't have a screener / scanner app that shows all of this information. Most of the contract interfaces we work with are in team repositories, and in some cases contract interfaces are opaque (for certain closed-source AMM / order-book exchange contracts).

### ServiceÂ Boundaries

* **Dedicated API vs Indexer**Â â€” REST/WebSocket traffic runs in its own process while chain ingestion remains isolated; this separation simplifies scaling decisions (CPU for workers, I/O for API) and keeps failure domains narrow.
* **Worker Isolation & Scaling **Â â€” Enrichment is CPUâ€‘bound and parallelâ€‘friendly. [BullMQ concurrency](https://docs.bullmq.io/guide/workers/concurrency) scaling makes each worker handle more jobs concurrently when the queue is busy. Alternatively you could use DockerÂ Swarm (or another orchestration tool) to do worker scaling, have a hook to watch queue depth metrics via Prometheus, and scale according.

### InternalÂ Communication

* **HTTP vs gRPC**Â â€” In production Iâ€™ve used gRPC for internal hops because the Protobuf contract gives compileâ€‘time typeâ€‘safety, you get lean binary payloads, and builtâ€‘in streaming on a single HTTP/2 connection. The schema codeâ€‘gen (generating protobuf files) step is a extra chore, and you need to respect backwards-compatibility through appending of new fields. For this codingâ€‘challenge prototype I kept the indexerâ†’queue call as simple HTTP/JSON.

### Realâ€‘TimeÂ Delivery

* **PostgreSQL LISTEN/NOTIFY** ensures notifications fire only after the row is committedÂ â€” no dualâ€‘write race that sometimes occurs with RedisÂ Pub/Sub.
* **WebSocket in Hono vs dedicated gateway**Â â€” Honoâ€™s WS support works but needs explicit upgrade handling. For production, a separate realtime service (Express with native `ws`, Fastify, uWebSockets.js) would simplify longâ€‘lived connection management, and keep the REST layer stateless.
* **Stateful connections** â€” The API keeps every WebSocket client in RAM, with a single replica thatâ€™s okay, but scaling vertically would require one of a few things:
â€¢ Connection affinity (sticky sessions). Let the L7 proxy hash on a stable token (cookie, auth header, or URL param) so each socket always lands on the same pod. If the pod dies every socket drops, and you canâ€™t scale below the number of affinity buckets.
â€¢ Shared broker / pub+sub (Redis / NATS / Kafka). Every pod publishes the event, and independently fan-outs to its own clients. Replicas publish a headline, and every gateway instance fanâ€‘outs to its local clients.  You can get zeroâ€‘downtime rolling deploys, and scaling at the cost of one more infra component.
* **Subscription filtering & rateâ€‘limits**Â â€” As an enhancement, you could support consumers sending a subscription message with filters (e.g nounId, eventType, etc.). The API would store those predicates per socket, and forward only matching events. EIPâ€‘4361 auth tokens gate users from the websocket, what a client can subscribe to, and a small tokenâ€‘bucket guards against subscribeâ€‘spamming.

### UpsertÂ vsÂ Update

* **Current flow:** Indexer performs the insert via Ponderâ€™s StoreÂ API, which buffers writes inâ€‘memory during historical sync and flushes them to PostgreSQL with COPY. Workers GET + UPDATE the event from the job data. This keeps insert logic in one placeÂ â€” the component that knows how to handle the re-orgsÂ â€” and avoids crossâ€‘service contention.

* **Scaling:** In production, more worker replicas (Docker ComposeÂ â–¶Â `--scale workers=N`, ECS service, etc.) consume the same queue. Because every job ultimately resolves to a single GET + UPDATE, contention is fairly minimal, and database locks remain shortâ€‘lived. BullMQâ€™s [batch jobs](https://docs.bullmq.io/bullmq-pro/batches) / pipelines could help by letting each worker acknowledge multiple completed jobs in one roundâ€‘trip.

* **Gripe with Ponder Drizzle API** StoreÂ API is part of Ponderâ€™s runtime and only available inside its hooks; the drizzle it exposes is readâ€‘only and lacks listen/notify. The API and workers run as a standalone Node service with their own connection pool; the API uses drizzle + PG for listen/notify, and the workers use raw SQL.

### Observability

* **Prometheus metrics** already exposed by Indexer and Queue/Worker.
* **Structured logging**Â â€” Used a replica of Ponderâ€™s internal logger using pino just for consistency / convenience, logging pretty for the purpose of the demo, but would use JSON for making aggregation in Loki or Elasticsearch (or any other solution) straightforward.

---

## ğŸ› ï¸Â MakefileÂ Commands

Below are the **userâ€‘facing commands** youâ€™ll run most often; full recipes are in each Makefile.

| Command             | Mode     | Description                                                                                |
| ------------------- | -------- | ------------------------------------------------------------------------------------------ |
| `make start`        | **Prod** | Build and launch the complete stack (all servicesÂ +Â infra)                                 |
| `make dev`          | **Dev**  | Spin up PostgresÂ &Â Redis only; you then run one or more `make devâ€‘<service>` targets below |
| `make dev-indexer`  | **Dev**  | Start the indexer with liveâ€‘reload, relying on the shared infra from `make dev`            |
| `make dev-api`      | **Dev**  | Start the RESTÂ +Â WebSocket API                                                             |
| `make dev-workers`  | **Dev**  | Launch the worker poolÂ &Â queue API                                                         |
| `make dev-frontend` | **Dev**  | Start the React frontâ€‘end (Vite dev server)                                                |
| `make test`         | **Test** | Endâ€‘toâ€‘end Foundry run (fork mainnet, deploy contracts, assert indexing)                   |
| `make stop`         | â€”        | Bring down all running containers                                                          |
| `make clean`        | â€”        | Stop containers and prune Docker artefacts                                                 |

For contractâ€‘level tests the **Foundry Makefile** offers:

| Command            | Purpose                                                            |
| ------------------ | ------------------------------------------------------------------ |
| `make start-anvil` | Spin up an **Anvil** mainnet fork (sideÂ stack)                     |
| `make deploy`      | Build a small deployer image and simulate a complete auction cycle |
| `make test-ponder` | Run Ponder against that fork and verify events                     |
| `make test`        | Shortcut: `start-anvilÂ â–¶Â deployÂ â–¶Â test-ponder`                     |
